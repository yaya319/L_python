{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "c91de82a4fe9611a4297cdf4f03c3b2ce54960da",
        "_cell_guid": "22be7b91-4c83-4ffc-afe7-bd078f017f31",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# -*- coding: utf-8 -*-\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy import stats\n\n#load data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1c2cfdebf24bc0fc3db9b657698e6fb1fed32f1",
        "_cell_guid": "a874994a-5bad-4485-b360-e41f49f4b78f"
      },
      "cell_type": "markdown",
      "source": "The data exploration part was inspired by this kernel: \nhttps://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n    "
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "51346b16831fc7fb86a35ae6f15b22177112c2c0",
        "_cell_guid": "76d6280e-cd60-474a-aef6-b8757d594a66",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Only for numeric variables\ndf_train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "fac997a67a7d53566c5d7f5700ff53703aff10ae",
        "_cell_guid": "b10cea74-3b61-4e70-a70b-5bffd035d50c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#scatter plot OverallQual/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c631606c2d4a95ec534933ee36c42e219d8a002f",
        "_cell_guid": "7a0de706-ba3d-486c-920b-373ef4868a0f"
      },
      "cell_type": "markdown",
      "source": "****The boxplot shows that the price spread is larger when the Overall Quall is larger"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "3e45bc22c2e21b1db0f6137f5312a08d39795ab9",
        "_cell_guid": "3a16d8f2-50fd-4a29-bd36-f957af71b053",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#box plot Neighborhood/saleprice\nvar = 'Neighborhood'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2a48b0c0898fe2b5712c81580bbacc5e4eee311",
        "_cell_guid": "18d306e6-7427-4ed1-9021-dd1144518648"
      },
      "cell_type": "markdown",
      "source": "****Some neighborhoods have higher average price than the others, for exemple: Nridght and StoneBr"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "69b7d0b68011e06853afa72f1a0ed3ab37b31169",
        "_cell_guid": "a18c0195-9650-479f-a1f4-32eb2ce7dd15",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "52e2efd3f37fcca99c332caec8f7e2bb41ff6522",
        "_cell_guid": "1069f020-637b-42aa-8711-db783ac4468b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# GarageYrBlt and YearBuilt \n#TotalBsmtSF  1stFlrSF  2ndFlrSF*\n#GarageArea and GarageArea\ncorrmat[abs(corrmat['SalePrice']>0.5)]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9c8723e88c55be5303b1df0ffe514b2100fa127a",
        "_cell_guid": "c10bf888-be12-439e-9ed2-7acca4644fda",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#select varaible: OverallQual YearBuilt YearRemodAdd TotalBsmtSF GrLivArea FullBath TotRmsAbvGrd GarageCars\nfeature=['SalePrice', 'OverallQual', 'GrLivArea', 'YearRemodAdd','GarageCars', 'TotRmsAbvGrd', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ndf_train_se=df_train[feature]\n#missing data\ntotal = df_train_se.isnull().sum().sort_values(ascending=False)\npercent = (df_train_se.isnull().sum()/df_train_se.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndf_train_se = df_train_se.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train_se = df_train_se.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train_se.isnull().sum().max() #just checking that there's no missing data missing...\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ac35c7e0f5eecbaa63c3e98079718a28139f6d63",
        "_cell_guid": "e7b1ba44-7ea0-4a97-affb-90c071af1a6b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#applying log transformation: because sale price is not normal distribution\ndf_train_se['SalePrice']=np.log(df_train_se['SalePrice'])\n#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndef trans(inputdata, var1):\n    inputdata['HasBsmt'] = pd.Series(len(inputdata[var1]), index=inputdata.index)\n    inputdata['HasBsmt'] = 0 \n    inputdata.loc[inputdata[var1]>0,'HasBsmt'] = 1\n    inputdata.loc[inputdata['HasBsmt']==1,var1] = np.log(inputdata[var1])\n    return inputdata\n\ntrans(df_train_se,'TotalBsmtSF')\ntrans(df_train_se,'GrLivArea')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "00f7b07217792be8f48410bc447046d00f9ef8cb",
        "_cell_guid": "b87e85fa-d710-4960-af7a-41cfcd6809d0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#transformed histogram and normal probability plot\ndef plot(var):\n    sns.distplot(df_train_se[var], fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(df_train_se[var], plot=plt)\n    return (res)\n\nplot('SalePrice')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d60be24cd280914d4c55f913075d166a6f8d21a9",
        "_cell_guid": "16c0611f-fffa-4b26-bf75-0abef351ff87",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#histogram and normal probability plot\nsns.distplot(df_train_se[df_train_se['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train_se[df_train_se['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "65455a6155bdb3ed353b4cfaba667c5d45ec9b92",
        "_cell_guid": "3d74d29f-8652-4be3-bef7-28dea6875237",
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature0=['OverallQual', 'GrLivArea', 'YearRemodAdd','GarageCars', 'TotRmsAbvGrd', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\ndf_train_X=df_train_se[feature0]\ndf_train_Y= df_train_se['SalePrice']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0ec47af5a9b6faff6219c2595748cd526e50d5a6",
        "_cell_guid": "ca172a6a-a2db-49ee-b466-0bc47194b086",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#1.Linear Regression\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Create linear regression object\nregr = linear_model.LinearRegression()\n# Train the model using the training sets\nregr.fit(df_train_X, df_train_Y)\n\n# Make predictions using the testing set\ndf_train_pred = regr.predict(df_train_X)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(df_train_Y, df_train_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(df_train_Y, df_train_pred))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "c97e3384ea274a6c4a42350eb898702429ce9f17",
        "_cell_guid": "9831d96a-3913-455e-9028-3f90254168fe",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#2. Ridge regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(df_train_X, df_train_Y,\n                                                   random_state = 0)\n\nlinridge = Ridge(alpha=20.0).fit(X_train, y_train)\n\nprint('House Price Pr√©diction')\nprint('ridge regression linear model intercept: {}'\n     .format(linridge.intercept_))\nprint('ridge regression linear model coeff:\\n{}'\n     .format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'\n     .format(linridge.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'\n     .format(linridge.score(X_test, y_test)))\nprint('Number of non-zero features: {}'\n     .format(np.sum(linridge.coef_ != 0)))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "18ab5abdbb722543a1440fee67975d4d08b974ca",
        "_cell_guid": "63aa3729-23ac-4fc1-b946-e9f352daa17b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#3. Lasso regression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train, X_test, y_train, y_test = train_test_split(df_train_X, df_train_Y,\n                                                   random_state = 0)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlinlasso = Lasso(alpha=0.001, max_iter = 10000).fit(X_train, y_train)\n\nprint('Crime dataset')\nprint('lasso regression linear model intercept: {}'\n     .format(linlasso.intercept_))\nprint('lasso regression linear model coeff:\\n{}'\n     .format(linlasso.coef_))\nprint('Non-zero features: {}'\n     .format(np.sum(linlasso.coef_ != 0)))\nprint('R-squared score (training): {:.3f}'\n     .format(linlasso.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}\\n'\n     .format(linlasso.score(X_test, y_test)))\nprint('Features with non-zero weight (sorted by absolute magnitude):')\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0543c1ef95becab0e808fb5389aad87dd8055033",
        "_cell_guid": "e2554f11-e981-45c7-b74e-81e0c8888716",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#4. GBM\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\nmse = mean_squared_error(y_test, clf.predict(X_test))\nprint(\"MSE: %.4f\" % mse)\n# Plot training deviance\n\n# compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\n\n# #############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nfeature_importance\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
